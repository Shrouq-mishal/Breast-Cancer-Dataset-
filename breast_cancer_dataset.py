# -*- coding: utf-8 -*-
"""Breast Cancer Dataset

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D6ExoZ_BHGtT3gSDQ349ZzRYP1zaBCPE

# Importing dataset ans libraries
"""

from sklearn.datasets import load_breast_cancer
import pandas as pd
import numpy as np
DSS=load_breast_cancer()
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.metrics import*
import matplotlib.pyplot as plt
import seaborn as sns

"""#Q1-What are the features, i.e. column names, of the dataset?


"""

#Load the Breast Cancer Dataset
data = load_breast_cancer()
features = data.feature_names
# Extract the column names
feature_names = data.feature_names
print("Column Names:")
print(feature_names)
print("_________________________________________________\n")

"""# Q2-What are the means and standard deviations for each of the features?"""

# loading the data to a dataFrame
df = pd.DataFrame(data.data, columns = data.feature_names)
# 'means' now contains means of each feature, and 'std_devs' contains standard deviations.
means = df.mean()
std_devs = df.std()
print("Means:")
print(means)
print("_________________________________________________\n")
print("Standard Deviations:")
print(std_devs)
print("_________________________________________________\n")

"""# Q3-What features are normally distributed?

"""

# Check if the selected features are normally distributed
for x in df.columns:
    mean = df[x].mean()
    if mean > 0.05:
        print(f"{x} => is not normally distributed.\n")
    else:
        print(f"{x} => is normally distributed\n")


# Create subplots for the histograms
features= ["mean radius","concave points error","mean smoothness"]
fig, axes = plt.subplots(len(features), 1, figsize=(8, 12))

for i, feature in enumerate(features):
    # Plot histograms
    sns.histplot(data=df, x=feature, kde=True, ax=axes[i])
    axes[i].set_title(f'Histogram of {feature}')

plt.tight_layout()
plt.show()

"""
# Q4-What features can be reduced, i.e. deleted, without affecting the model performance?"""

# Create a DataFrame from the dataset
df = pd.DataFrame(data.data, columns=data.feature_names)
df['Diagnosis'] = data.target

# Split the data into features (X) and target variable (y)
X = df.drop('Diagnosis', axis=1)
y = df['Diagnosis']

# Perform feature selection using ANOVA F-value
selector = SelectKBest(score_func=f_classif, k='all')
selector.fit(X, y)
# Get the p-values and scores for each feature
feature_scores = pd.DataFrame({'Feature': X.columns, 'Score': selector.scores_, 'p-value': selector.pvalues_})
# Sort the features by p-value in ascending order
feature_scores = feature_scores.sort_values(by='p-value')
# Set a threshold for p-value (e.g., 0.05)
threshold = 0.05
# Identify features with p-value above the threshold
reduced_features = feature_scores[feature_scores['p-value'] > threshold]['Feature'].tolist()
# Print the features that can be reduced
print("_________________________________________________\n")
print("Features that can be reduced:\n")
print(reduced_features)

"""
#Q5-What are the possible predictions and how many examples are there in the dataset for each of the predictions?"""

print("____________________________________")
target_names=data.target_names
target_values, target_counts = np.unique(data.target, return_counts=True)
print("The possible predictions and examples:")

for target, count in zip(target_values, target_counts):
    prediction = target_names[target]
    print(f"Prediction: {prediction}, Examples: {count}")

"""#Q6-Split the data into training and testing data."""

x=data.data
y=data.target

# Split the data into training and testing sets, 20% test data and 80% training data.
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=5)

print(" x_train shape:", x_train.shape) #Display the shape of the training features.
print(" x_test shape:", x_test.shape)   #========================= testing ========.
print(" y_train shape:", y_train.shape) #========================= training target variable.
print(" y_test shape:", y_test.shape)   #========================= testing ========.

"""#Q7-Train the models on the training data.

**1**-KNN model
"""

knn=KNeighborsClassifier(n_neighbors=2)
#fit
knn.fit(x_train,y_train)
#predict
pred=knn.predict(x_test)
#score
accuracy=accuracy_score(y_test,pred)
precision=precision_score(y_test,pred)
recall=recall_score(y_test,pred)
f1=f1_score(y_test,pred)
auc=roc_auc_score(y_test,pred)
#print the model's performance
print("\nAccuracy is:",accuracy_score(y_test, pred))
print('______________________\n')
cm =confusion_matrix(y_test,pred)
sns.heatmap(cm,annot=True)
print(classification_report(y_test,pred))
print(accuracy_score(y_test,pred))

"""2-Gaussian
Naive Bayes


"""

gnb=GaussianNB()
#fit
gnb.fit(x_train,y_train)
#predict
Gpred =gnb.predict(x_test)
#score
accuracy=accuracy_score(y_test,Gpred)
precision=precision_score(y_test,Gpred)
recall=recall_score(y_test,Gpred)
f1=f1_score(y_test,Gpred)
auc=roc_auc_score(y_test,Gpred)
#print the model's performance
print("\nAccuracy is:",accuracy_score(y_test, Gpred))
print('______________________\n')
cm =confusion_matrix(y_test,Gpred)
sns.heatmap(cm,annot=True)
print(confusion_matrix(y_test,Gpred))
print(classification_report(y_test,Gpred))
print(accuracy_score(y_test,Gpred))

"""

```
# This is formatted as code
```

3-Polynomial Regression"""

from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(solver = 'newton-cg',random_state=200)
#fit
classifier.fit(x_train, y_train)
#predict
y_pred = classifier.predict(x_test)
#print the model's performance
print("\nAccuracy is:",accuracy_score(y_test, y_pred))
print('______________________\n')
cm =confusion_matrix(y_test,y_pred)
sns.heatmap(cm,annot=True)
print(classification_report(y_test,y_pred))
print(accuracy_score(y_test,y_pred))